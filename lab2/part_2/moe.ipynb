{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### 简介\n",
    "Tokenization 的主要目的是将文本分解成更小的单位 (Tokens)，减小模型输入数据的内在结构复杂度 (从句子变为单词序列)，从而简化模型训练的难度。同时将字符的序列转化为 Token 序号的序列，便于模型输入。\n",
    "\n",
    "Tokenization 首先确定语言的词表划分粒度，一般可分为：\n",
    "* 字符级：将文本分解为字符。\n",
    "* 单词级：将文本分解为单词。\n",
    "* 子词级：将单词进一步分解为更小的有意义单元（如前缀、后缀）。\n",
    "\n",
    "之后使用预定义的规则来识别 tokens, 或使用统计或机器学习技术来识别最优的 token 切分方式。例如，BPE（Byte Pair Encoding）或 SentencePiece。\n",
    "\n",
    "最后实现一组文本序列和 Tokens 序列之间相互转化的函数，即可完成 Tokenization 部分。\n",
    "\n",
    "### 实验要求\n",
    "\n",
    "1. 实现字符级切分的简单 tokenizer， 由 字符表， 字符到 token 的 encoder () 函数 和 token 到字符的 decoder () 函数组成。\n",
    "2. 调用 现有的 tokenizer 实现，比如 openai 的 tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, dataPath: str):\n",
    "        with open(dataPath, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.dataset = f.read()\n",
    "        self.generate_vocabulary()\n",
    "\n",
    "    def generate_vocabulary(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        # FIXME:\n",
    "        # Create a sorted list of unique characters\n",
    "        unique_chars = sorted(set(self.dataset))\n",
    "        # Create a dictionary to map each character to a unique index\n",
    "        self.char2index = {char: idx + 1 for idx, char in enumerate(unique_chars)}\n",
    "        self.char2index[\"<START>\"] = 0  # Adding start token\n",
    "        self.char2index[\"<END>\"] = len(self.char2index)  # Adding end token\n",
    "        # Create a dictionary to map each index back to its character\n",
    "        self.index2char = {idx: char for char, idx in self.char2index.items()}\n",
    "\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        sentence: str,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        # FIXME:\n",
    "        例子, 假设 A-Z 对应的 token 是 1-26, 句子开始，结束符号的 token 是 0。\n",
    "        input  : \"ABCD\"\n",
    "        output : Tensor([0,1,2,3])\n",
    "\n",
    "        注意: 为了后续实验方便，输出 Tensor的 数据类型 dtype 为 torch.long。\n",
    "        \"\"\"\n",
    "        # Encode the sentence to a list of indices\n",
    "        tokens = [self.char2index[\"<START>\"]] + [self.char2index[char] for char in sentence] + [self.char2index[\"<END>\"]]\n",
    "        # Convert the list to a tensor of type torch.long\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        tokens: torch.Tensor,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        # FIXME:\n",
    "        例子, 假设 A-Z 对应的 token 是 1-26, 句子开始，结束符号的 token 是 0。\n",
    "        input : Tensor([0,1,2,3])\n",
    "        output : \"ABCD\"\n",
    "        \"\"\"\n",
    "        # Decode the tensor of indices back to a string\n",
    "        chars = [self.index2char[idx.item()] for idx in tokens if idx.item() in self.index2char]\n",
    "        # Join the characters to form the decoded string\n",
    "        return \"\".join(chars).replace(\"<START>\", \"\").replace(\"<END>\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 dataloader 和 dataset\n",
    "\n",
    "为了高效加载数据，我们需要把输入文件接入 PyTorch 的数据加载器中。在这里我们定义 `ShakespeareDataset` 类用于加载数据集，用 PyTorch 的 `DataLoader` 类来实现数据加载。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, filepath, tokenizer, chunk_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "        self.encoded = self.tokenizer.encode(text)\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded) - self.chunk_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # FIXME:提取一段文本(长度为 chunk_size）作为输入，以及这段文本的每一个字符的下一个字符作为标签\n",
    "        # example(not correspond to real text): chunk = tensor([ 0, 20, 49, 58, 59])\n",
    "        #         label = tensor([20, 49, 58, 59, 19])\n",
    "        # decoded chunk: \"The \"\n",
    "        # decoded label: \"he T\"\n",
    "        chunk = self.encoded[idx:idx + self.chunk_size]\n",
    "        label = self.encoded[idx + 1:idx + self.chunk_size + 1]\n",
    "        return chunk, label\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(dataPath=\"input.txt\")\n",
    "\n",
    "\n",
    "def create_dataloader(filepath, tokenizer, chunk_size, batch_size, shuffle=True):\n",
    "    dataset = ShakespeareDataset(filepath, tokenizer, chunk_size)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [int(len(dataset) * 0.8), len(dataset) - int(len(dataset) * 0.8)]\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意力的计算公式为：\n",
    "$$\n",
    "Head = Attention(x)=Softmax(M\\cdot QK^T)V\\\\\n",
    "Q=xW_{q},K=xW_{k}, V=xW_{v}\n",
    "$$\n",
    "这里实现的一些数学技巧可以参见 `attention.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadAttention(nn.Module):\n",
    "    def __init__(self, seq_len: int, embed_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        # embed_size: dimension for input embedding vector\n",
    "        # hidden_size: dimension for hidden vector. eg. x:(..., embed_size) --to_q--> query_vector:(..., hidden_size)\n",
    "\n",
    "        # a triangular bool matrix for mask\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(seq_len, seq_len)))\n",
    "\n",
    "        # FIXME:\n",
    "        # Initialize three linear transformations for Q, K, and V\n",
    "        self.to_q = nn.Linear(embed_size, hidden_size)\n",
    "        self.to_k = nn.Linear(embed_size, hidden_size)\n",
    "        self.to_v = nn.Linear(embed_size, hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, inputs) -> torch.Tensor:\n",
    "        # input: (batch_size, seq_len, embed_size)\n",
    "        # return (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # FIXME:\n",
    "        batch_size, seq_len, embed_size = inputs.shape\n",
    "\n",
    "        # Apply linear transformations to get Q, K, V\n",
    "        Q = self.to_q(inputs)  # (batch_size, seq_len, hidden_size)\n",
    "        K = self.to_k(inputs)  # (batch_size, seq_len, hidden_size)\n",
    "        V = self.to_v(inputs)  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch_size, seq_len, seq_len)\n",
    "        scores = scores / (embed_size ** 0.5)  # Scale scores\n",
    "\n",
    "        # Apply mask\n",
    "        mask = self.tril[:seq_len, :seq_len]  # Ensure mask has the same size as scores\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # (batch_size, seq_len, seq_len)\n",
    "\n",
    "        # Multiply attention weights with V\n",
    "        out = torch.matmul(attn_weights, V)  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 中使用的注意力机制时会使用多个注意力头，期望每个注意力头能够注意到不同的信息。\n",
    "所以实际公式需要修改如下\n",
    "$$\n",
    "MultiHeadAttention(x)=[Head_0, Head_1,...,Head_h]W_o\\\\\n",
    "Head_i = Attention(x)=Softmax(M\\cdot Q_iK_i^T)V_i\\\\\n",
    "Q_i=xW_{iq},K=xW_{ik}, V=xW_{iv}\n",
    "$$\n",
    "在搭建网络的过程中，同学们可能会用到 `nn.ModuleList` 这个库，每个 $Head_i$ 的计算可以直接使用上面已经实现的单头注意力计算。最后对于这些注意力头再使用一个简单的线性层/矩阵 $W_o$ 汇总信息即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # MultiHeadAttention is consist of many HeadAttention output.\n",
    "    # concat all this head attention output o_i, then merge them with a projection matrix W_o, as [o_1, o_2, ...] x W_o\n",
    "    # The reason for using multi-head attention is that we want each head to be able to extract different features\n",
    "    def __init__(self, n_heads: int, head_size: int, seq_len: int, embed_size: int):\n",
    "        # n_heads is the number of head attention\n",
    "        # head_size is the hidden_size in each HeadAttention\n",
    "        super().__init__()\n",
    "\n",
    "        # FIXME:\n",
    "        # head_size = embed_size // n_heads\n",
    "\n",
    "        # Ensure embed_size is divisible by n_heads\n",
    "        assert embed_size % n_heads == 0, \"embed_size must be divisible by n_heads\"\n",
    "\n",
    "        # Define the head size based on the number of heads and embedding size\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = head_size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # Create multiple HeadAttention modules and store them in a ModuleList\n",
    "        self.heads = nn.ModuleList(\n",
    "            [HeadAttention(seq_len, embed_size, head_size) for _ in range(n_heads)]\n",
    "        )\n",
    "\n",
    "        # Define the linear layer to combine the outputs from all heads\n",
    "        self.linear = nn.Linear(n_heads * head_size, embed_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size), make sure embed_size=n_heads x head_size\n",
    "        # return: (batch_size, seq_len, embed_size)\n",
    "\n",
    "        # FIXME:\n",
    "        # Apply each HeadAttention module to the inputs\n",
    "        head_outputs = [head(inputs) for head in self.heads]  # List of (batch_size, seq_len, head_size)\n",
    "\n",
    "        # Concatenate the outputs from each head along the last dimension\n",
    "        concatenated = torch.cat(head_outputs, dim=-1)  # (batch_size, seq_len, n_heads * head_size)\n",
    "\n",
    "        # Apply the linear layer to combine the heads' outputs\n",
    "        output = self.linear(concatenated)  # (batch_size, seq_len, embed_size)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 专家网络 Expert\n",
    "\n",
    "Expert 即为标准 Transformer 中的 FeedForward 模块。\n",
    "\n",
    "在经过 MultiHeadAttention 模块后，seq_len 中的每一个 Embedding 都对应了前文信息的加权求和。在经过 FeedForward 模块时，模型对每一个位置的 Embedding 进行了两次线性变换和一次非线性变换，可以视为对当前语境下的信息进行加工。知识编辑的一些研究表明，FeedForword 模块参数包含了大量的事实性知识。\n",
    "\n",
    "一个直观的想法是，类比于 MultiHeadAttention，我们在每一层训练多个 FeedForward 模块，对于不同位置的 Embedding 使用不同的 FeedForward 模块处理对应的信息。就好像每层有多个 Expert, 每个 Expert 都负责处理一类数据的深加工，因此我们称 FeedForward 为 Expert。\n",
    "\n",
    "实现方面:\n",
    "\n",
    "FeedForward 层由两层简单的线性层组成，对于一个 (batch_size, seq_len, embed_size) 输入的向量 x\n",
    "只在最后一个维度上进行计算，以实现词的特征维度上的交互 (注意力机制是词之间的交互)。\n",
    "其首先用一个线性层将 x 最后一维扩大至原先 4 倍，然后继续用一个线性层还原回原先的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, embed_size: int):\n",
    "        super().__init__()\n",
    "        # FIXME:\n",
    "        self.linear1 = nn.Linear(embed_size, 4 * embed_size)\n",
    "        self.linear2 = nn.Linear(4 * embed_size, embed_size)\n",
    "        self.activation = nn.ReLU()  # Use ReLU as the activation function\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: (batch_size, seq_len, embed_size)\n",
    "        # -> mid: (batch_size, seq_len, 4 x embed_size)\n",
    "        # -> outputs: (batch_size, seq_len, embed_size)\n",
    "        # FIXME:\n",
    "        # Apply the first linear transformation and activation\n",
    "        mid = self.activation(self.linear1(inputs))  # (batch_size, seq_len, 4 * embed_size)\n",
    "        # Apply the second linear transformation\n",
    "        outputs = self.linear2(mid)  # (batch_size, seq_len, embed_size)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选通网络 TopkRouter\n",
    "\n",
    "在实现了单个 Expert 后，我们要设计一个选通网络决策每个 Embedding 要使用那个 Expert 计算\n",
    "\n",
    "### 为了说明选通网络的实现方式，我们定义一下记号：\n",
    "\n",
    "inputs.shape = [batch_size, seq_len, embed_size] = [1, 8, 16]\n",
    "\n",
    "即输入有 batch_size=1 个数据点，该数据有 seq_len 长度的 context，即包含 seq_len=8 个 Embedding，每个 Embedding 长度为 embed_dim=16。\n",
    "\n",
    "记 num_expert=4, 即该层包含 num_expert 个并列的 Expert。\n",
    "\n",
    "记 active_expert=2, 即计算每个 Embedding 仅有 active_expert 个 Expert 参与计算。\n",
    "\n",
    "### 选通网络计算\n",
    "对于有 seq_len=8 的数据，如果每个 Expert 都参与计算每一个 Embedding，那么一共需要计算 seq_len*embed_size=32 次， 这极大的增加了模型计算量，因此我们往往只激活其中的 active_experts 个 Expert，这要求我们对每一个 Embedding 计算最合适的 active_experts 个 Expert。\n",
    "\n",
    "对于单个 Expert 的原版 Transformer 来说：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = FeedForward(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "对于多个 Expert 的网络：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = \\sum_{i \\in range(num\\_model)} \\alpha_{i} Expert_{i}(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_{i} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    1 & Expert_{i}  \\text{is selected} \\\\\n",
    "    0 & Expert_{i}  \\text{is not selected} \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "将 $\\{\\alpha_0,\\alpha_1,\\dots,\\alpha_{num_experts-1}\\}$ 记为向量 $\\alpha$:\n",
    "$$\n",
    "outputs[0,seq] = \\alpha \\cdot \\{Expert_i(inputs[0,seq])\\}\n",
    "$$\n",
    "\n",
    "一个选通 0,2 号 Expert 的 $\\alpha$ 的例子是 $[1,0,1,0]$\n",
    "\n",
    "问题在于如何求得 $\\alpha$, 对于一个 Embedding，我们使用神经网络对每个 Expert 打分，再根据分数计算 $\\alpha$\n",
    "\n",
    "$$\n",
    "score[0,seq] = MLP(inputs[0,seq])  \\\\\n",
    "\\alpha = topK(score[0,seq])\n",
    "$$\n",
    "\n",
    "例如：\n",
    "\n",
    "$$\n",
    "score[0,seq] = [11.32,1.54,14.83,-1.90] \\\\\n",
    "\\alpha = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "从优化的角度来说，$\\alpha$ 取前 $k$ 大的分数的下标（即 argmax），这个操作是不可导的，这里我们用之前在 `attention.ipynb` 中提到的技巧处理这里的计算。\n",
    "\n",
    "$$\n",
    "mask(score[0,seq]) = [11.32,-inf,14.83,-inf] \\\\\n",
    "\\alpha = softmax(mask(score[0,seq])) = [0.028,0,0.971,0] \\\\\n",
    "index = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "我们用这个 $\\alpha$ 和 $index$ 用做选通网络."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First define the top k router module\n",
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, embed_size, num_experts, active_experts):\n",
    "        ## FIXME:\n",
    "        ## embed_size : dimension of embedding\n",
    "        ## num_experts : how many Experts per layer\n",
    "        ## active_experts: only active_experts out of num_experts are selected to process Embeddings per token.\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "\n",
    "        # Define a linear layer to compute scores for each expert\n",
    "        self.score_network = nn.Linear(embed_size, num_experts)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## FIXME:\n",
    "        ## 完成这部分时，注意使用 Softmax() 对 router_output 做标准化。同时注意这部分所用操作的可导性。\n",
    "        ## 输入值\n",
    "        ## inputs is the output tensor from multihead self attention block, shape (B:batch size, T: seq_len, C: embed_size)\n",
    "        ## 返回值\n",
    "        ## router_output: normalized weight of Experts, 即教程中的 \\alpha\n",
    "        ## indices:   index of selected Experts, 即教程中的 index\n",
    "        # Compute scores for each expert\n",
    "        scores = self.score_network(inputs)  # (batch_size, seq_len, num_experts)\n",
    "\n",
    "        # Get the top k scores and their indices\n",
    "        topk_scores, topk_indices = torch.topk(scores, self.active_experts, dim=-1)\n",
    "\n",
    "        # Create a mask with -inf for non-topk scores\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        mask.scatter_(-1, topk_indices, topk_scores)\n",
    "\n",
    "        # Apply softmax to the masked scores to get normalized weights\n",
    "        router_output = F.softmax(mask, dim=-1)  # (batch_size, seq_len, num_experts)\n",
    "\n",
    "        # Create binary indices for selected experts\n",
    "        indices = torch.zeros_like(scores)\n",
    "        indices.scatter_(-1, topk_indices, 1)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 稀疏专家网络 SparseMoE\n",
    "\n",
    "![moe](./moeSparse.png)\n",
    "\n",
    "在定义完 Expert 和 TopkRouter 后，我们可以定义 SparseMoE 模块。\n",
    "\n",
    "在前向过程中，对于 inputs.shape=[Batch_size,seq_len,embed_size] 第二维度 seq_len 个 Embedding,我们先利用 TopkRouter 计算出选通专家序号 indices 以及专家权重 router_output。\n",
    "\n",
    "我们将 Embedding 通过选通的 Expert 得出 active_expert 个新的 Embedding，然后使用 router_output 的作为权重对新的 Embedding 加权求和作为输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, embed_size: int, num_experts: int, active_experts: int):\n",
    "        # FIXME:\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "\n",
    "        # Initialize the TopkRouter\n",
    "        self.router = TopkRouter(embed_size, num_experts, active_experts)\n",
    "\n",
    "        # Initialize the Experts\n",
    "        self.experts = nn.ModuleList([Expert(embed_size) for _ in range(num_experts)])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # FIXME:\n",
    "        # inputs: (batch_size, seq_len, embed_size)\n",
    "        # Get the router outputs and indices of selected experts\n",
    "        router_output, indices = self.router(inputs)  # (batch_size, seq_len, num_experts)\n",
    "        # Initialize the final output tensor\n",
    "        batch_size, seq_len, _ = inputs.shape\n",
    "        final_output = torch.zeros(batch_size, seq_len, self.embed_size, device=inputs.device)\n",
    "\n",
    "        # Iterate over the number of experts\n",
    "        for i in range(self.num_experts):\n",
    "            # Get the indices where the i-th expert is active\n",
    "            mask = indices[..., i]  # (batch_size, seq_len)\n",
    "            if mask.sum() > 0:\n",
    "                # Extract the embeddings that will go through the i-th expert\n",
    "                expert_inputs = inputs * mask.unsqueeze(-1)  # (batch_size, seq_len, embed_size)\n",
    "                # Apply the i-th expert\n",
    "                expert_output = self.experts[i](expert_inputs)  # (batch_size, seq_len, embed_size)\n",
    "                # Weight the expert output with the router output and accumulate\n",
    "                final_output += expert_output * router_output[..., i].unsqueeze(-1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 由一层层的 block 堆叠而成，其中每个 block 的结构从模型的结构图展开中可以看到，由 LayerNorm，Masked multi head attention，(SparseMoE) FeedForward 组成。\n",
    "\n",
    "对于一个表示句子的输入向量 x，其首先会经过 Layer Normalization 层。Layer Normalization 层对于一个句子个数 x 句子长度 x 单词向量维度的输入 x, 会在最后两维上进行规范化处理，起到稳定训练的作用。\n",
    "\n",
    "$$\n",
    "LN(x)=\\frac{x-mean(x)}{\\sqrt{var(x)+\\epsilon}}\\cdot\\gamma+\\beta\n",
    "$$\n",
    "\n",
    "其中 mean 和 var 都是在最后两个维度上进行的，layernorm 的实现同学们可以直接调用 nn.LayerNorm。经过 layernorm 层后，再经过 Mask multi head attention 层之后，会在 + 号处再次和原始的输入进行相加，这样的做法能够提高训练的稳定性。有兴趣的同学可以从梯度角度思考原因，或者搜索残差连接相关资料进行学习。之后再同样经过一层 layernorm 和 feedforwad 之后，就可以得到 block 块的输出了。即 x' = x+MHA(LN(x)), y = FFN(LN(x'))+x'。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # Transformer basic block, consist of MultiHeadAttention, FeedForward and layer normalization\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_size: int,\n",
    "        n_heads: int,\n",
    "        seq_len: int,\n",
    "        num_experts: int,\n",
    "        active_experts: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # FIXME: implement block structure\n",
    "        self.layernorm1 = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(n_heads, embed_size // n_heads, seq_len, embed_size)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_size)\n",
    "        self.feedforward = SparseMoE(embed_size, num_experts, active_experts)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size)\n",
    "        # FIXME: forward with residual connection\n",
    "        x = self.layernorm1(inputs)\n",
    "        x = self.attention(x) + inputs\n",
    "        y = self.layernorm2(x)\n",
    "        output = self.feedforward(y) + x\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoETransformer(nn.Module):\n",
    "    # Transformer decoder, consist of\n",
    "    # token embedding layer and position_embedding(position_embedding 可以理解为对位置编码，感兴趣的同学可以查阅原文，这里可以看为vocab_len = seq_len的Embedding)\n",
    "    # a stack of Transformer basic block\n",
    "    # a layernorm and output linear layer\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        seq_len: int,\n",
    "        embed_size: int,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        num_experts: int,\n",
    "        active_experts: int,\n",
    "    ):\n",
    "        # vocab_size is the number of word in vocabulary dict\n",
    "        # seq_len is the sequence length/sentence length\n",
    "        # embed_size is the embedding vector dimension\n",
    "        super().__init__()\n",
    "        # FIXME:\n",
    "        self.seq_len = seq_len\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(seq_len, embed_size)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_size, n_heads, seq_len, num_experts, active_experts) \n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.layernorm = nn.LayerNorm(embed_size)\n",
    "        self.output_layer = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        # labels: the (ground) true output\n",
    "        # FIXME: implement the forward function of the transformer\n",
    "\n",
    "        # inputs:(batch_size, seq_len, )\n",
    "        batch_size, seq_len = inputs.shape\n",
    "        # embedding:(batch_size, seq_len, embed_size)\n",
    "        position_ids = torch.arange(seq_len, device=inputs.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        embeddings = self.token_embedding(inputs) + self.position_embedding(position_ids)\n",
    "        # attens:(batch_size, seq_len, embed_size)\n",
    "        x = embeddings\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        # logits:(batch_size, seq_len, vocab_size)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        # compute the loss\n",
    "\n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size * seq_len, vocab_size)\n",
    "            labels = labels.view(batch_size * seq_len)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        inputs = torch.tensor(tokenizer.encode(inputs)).unsqueeze(0)\n",
    "        device = next(self.parameters()).device\n",
    "        inputs = inputs.to(device)\n",
    "        if inputs.size(1) > self.seq_len:\n",
    "            inputs = inputs[:, : self.seq_len]\n",
    "        generated = inputs\n",
    "        for _ in range(max_new_tokens):\n",
    "            if generated.size(1) > self.seq_len:\n",
    "                generated_input = generated[:, -self.seq_len :]\n",
    "            else:\n",
    "                generated_input = generated\n",
    "            logits, _ = self.forward(generated_input)\n",
    "            last_logits = logits[:, -1, :]\n",
    "            next_token_ids = torch.argmax(last_logits, dim=-1)\n",
    "            next_token_ids = next_token_ids.unsqueeze(-1)\n",
    "            generated = torch.cat([generated, next_token_ids], dim=1)\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练循环\n",
    "\n",
    "如果你已经完成了模型定义等内容，训练的过程实际上在高度封装的 Pytorch 库中非常简单, 因为你并不需要写对应的反向传播。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss\n",
    "\n",
    "Loss 用来**衡量**模型预测与真实值之间的**差距**。\n",
    "\n",
    "常见的几个 Loss 函数：\n",
    "\n",
    "* 交叉熵：$\\text{CrossEntropy Loss} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)$\n",
    "* 均方误差：$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$\n",
    "* 绝对误差：$\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y_i}|$\n",
    "\n",
    "不同的 loss 对应不同的优化目标，如果写错 loss 函数会导致模型不收敛/性能很差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练循环\n",
    "\n",
    "当我们写好 Optimizer 和 Loss 之后，对应的训练循环就十分简单了。\n",
    "\n",
    "我们只需要做以下事情：\n",
    "\n",
    "* 从 dataloader 里面拿到一个 batch 的数据以及标签\n",
    "* 将数据送入模型，进行前向传播\n",
    "* 拿到模型输出的 logits\n",
    "* 将 logits 和 标签进行 loss 计算\n",
    "* 用 Optimizer\n",
    "    * 清空梯度\n",
    "    * 反向传播\n",
    "    * 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, dataloader, epoch, device):\n",
    "    # Optimizer 会根据模型的输出和真实标签计算梯度，然后利用反向传播算法更新模型的参数。\n",
    "    # 在本实验中你可以将 Optimizer 视作黑盒，只需要知道如何使用即可。\n",
    "    # 找一个合适的 Optimizer。对不同的任务，模型，最适合的优化器是不一样的，你可以先尝试最常用的 Adam，如果有兴趣可以看看其他的优化器。\n",
    "    # docs see: https://pytorch.org/docs/stable/optim.html\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # FIXME:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # FIXME: implement the training process, and compute the training loss and validation loss\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(inputs, labels=targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch} Loss: {total_loss / len(dataloader)}\")\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, epoch, device):\n",
    "    model.eval()\n",
    "    # FIXME: 实现验证函数。与训练函数类似，但不需要计算梯度。\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits, loss = model(inputs, labels=targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch} Validation Loss: {total_loss / len(dataloader)}\")\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:10<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 1.7492950986461673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:11<00:00, 37.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Loss: 1.416691155608641\n",
      "Epoch 0 Train Loss: 1.7492950986461673, Valid Loss: 1.416691155608641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:10<00:00, 13.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.360903606836097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:11<00:00, 36.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Loss: 1.3217352010788175\n",
      "Epoch 1 Train Loss: 1.360903606836097, Valid Loss: 1.3217352010788175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:09<00:00, 13.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.2939875394670155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:11<00:00, 38.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Loss: 1.2741222933891716\n",
      "Epoch 2 Train Loss: 1.2939875394670155, Valid Loss: 1.2741222933891716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:09<00:00, 13.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.2517616570987582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:11<00:00, 38.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Loss: 1.2385612581301173\n",
      "Epoch 3 Train Loss: 1.2517616570987582, Valid Loss: 1.2385612581301173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1743/1743 [02:09<00:00, 13.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 1.2202892826674523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 436/436 [00:11<00:00, 38.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Loss: 1.2101615918885678\n",
      "Epoch 4 Train Loss: 1.2202892826674523, Valid Loss: 1.2101615918885678\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1518753/2938726709.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(tokenizer.encode(inputs)).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I could pick my lanceond the world with the world,\n",
      "And then the common of the people,\n",
      "And then the common of the people,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader = create_dataloader(\"input.txt\", tokenizer, chunk_size=50, batch_size=512)\n",
    "model = SparseMoETransformer(\n",
    "    vocab_size=len(tokenizer.char2index),\n",
    "    seq_len=50,\n",
    "    embed_size=64,\n",
    "    n_layers=3,\n",
    "    n_heads=8,\n",
    "    num_experts=8,\n",
    "    active_experts=2,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "def run(model, train_dataloader, valid_dataloader, device, epochs=10):\n",
    "    train_losses = [] # FIXME:\n",
    "    valid_losses = [] # FIXME:\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_dataloader, epoch, device)\n",
    "        valid_loss = validate(model, valid_dataloader, epoch, device)\n",
    "        train_losses.append(train_loss) # FIXME:\n",
    "        valid_losses.append(valid_loss) # FIXME:\n",
    "        print(f\"Epoch {epoch} Train Loss: {train_loss}, Valid Loss: {valid_loss}\")\n",
    "\n",
    "    return train_losses, valid_losses\n",
    "\n",
    "\n",
    "# FIXME: 用 matplotlib plot 训练过程中的 loss 变化\n",
    "# run(model, dataloader, None, device, epochs=5)\n",
    "train_losses, valid_losses = run(model, train_dataloader, val_dataloader, device, epochs=5)\n",
    "# Plot the training and validation loss\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(valid_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(\"I could pick my lance\", max_new_tokens=100)[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1518753/2938726709.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(tokenizer.encode(inputs)).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A strange one as everome time\n",
      "The stret-place of the prince of the people,\n",
      "And then the common of the prince of the peopl\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(\"A strange one as ever\", max_new_tokens=100)[0]\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
